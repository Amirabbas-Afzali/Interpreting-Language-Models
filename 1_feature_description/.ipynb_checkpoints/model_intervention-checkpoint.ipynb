{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8e4483-6ea2-491b-8f55-751cc6fb9b5c",
   "metadata": {},
   "source": [
    "## Load the Target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b542818-8bff-48d7-b15e-a23158c7c0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f3e8ae4ca34fefbbe234dc79c71651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CACHE_DIR = \"/mnt/raid10/ak-research-01/ak-research-01/codes/.cache\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from contextlib import contextmanager\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# 1) Load Llama-3.1-8B (target/explainer model)\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "device = model.device\n",
    "dtype = next(model.parameters()).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "777a70ff-b46d-4f4f-afeb-f743f8c19045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Ensure [s] and [e] are special tokens\n",
    "special_tokens = {\"additional_special_tokens\": [\"[s]\", \"[e]\"]}\n",
    "\n",
    "need_add = any(tok not in tokenizer.get_vocab()\n",
    "               for tok in special_tokens[\"additional_special_tokens\"])\n",
    "if need_add:\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "ID_S = tokenizer.convert_tokens_to_ids(\"[s]\")\n",
    "ID_E = tokenizer.convert_tokens_to_ids(\"[e]\")\n",
    "\n",
    "# 3) Paper templates with [s]v[e]\n",
    "FEATURE_TEMPLATES = [\n",
    "    \"At layer {layer}, [s]v[e] encodes\",\n",
    "    \"[s]v[e] activates at layer {layer} for\",\n",
    "    \"We can describe [s]v[e] at layer {layer} as encoding\",\n",
    "    \"Generate a description of this feature at layer {layer}: [s]v[e].\",\n",
    "    \"What does [s]v[e] mean at layer {layer}?\",\n",
    "    \"[s]v[e] activates at layer {layer} for inputs with the following features:\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08a0383b-169c-4717-829b-9226d0abfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_prompt(layer: int, template_id: int | None = None) -> str:\n",
    "    if template_id is None:\n",
    "        template_id = random.randrange(len(FEATURE_TEMPLATES))\n",
    "    return FEATURE_TEMPLATES[template_id].format(layer=layer) + \" \"\n",
    "\n",
    "def normalize_feature(v: torch.Tensor) -> torch.Tensor:\n",
    "    v = v.to(device=device, dtype=dtype)\n",
    "    return v / (v.norm() + 1e-8)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def patch_at_v_between_s_e(model, input_ids: torch.Tensor, v: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Find a span [s] v [e] in the tokenized *prompt* and replace the embedding of\n",
    "    the token 'v' (the one between [s] and [e]) with the continuous vector v.\n",
    "    This patch is applied only on the first forward pass (full prompt),\n",
    "    and skipped on later generation steps (seq_len = 1).\n",
    "    \"\"\"\n",
    "    emb = model.get_input_embeddings()\n",
    "    v = v.to(device=emb.weight.device, dtype=emb.weight.dtype)\n",
    "\n",
    "    ids = input_ids[0]  # [seq_len] for the *prompt*\n",
    "\n",
    "    # locate [s]\n",
    "    s_positions = (ids == ID_S).nonzero(as_tuple=True)[0]\n",
    "    if len(s_positions) == 0:\n",
    "        raise ValueError(\"Prompt does not contain [s] token.\")\n",
    "    s_idx = int(s_positions[0].item())\n",
    "\n",
    "    # locate [e] AFTER [s]\n",
    "    e_positions = (ids == ID_E).nonzero(as_tuple=True)[0]\n",
    "    e_positions = e_positions[e_positions > s_idx]\n",
    "    if len(e_positions) == 0:\n",
    "        raise ValueError(\"Prompt does not contain [e] token after [s].\")\n",
    "    e_idx = int(e_positions[0].item())\n",
    "\n",
    "    # tokens strictly between [s] and [e] → expect just 'v'\n",
    "    mid_positions = list(range(s_idx + 1, e_idx))\n",
    "    if len(mid_positions) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected exactly one token between [s] and [e], got {len(mid_positions)}.\"\n",
    "        )\n",
    "    v_idx = mid_positions[0]\n",
    "\n",
    "    patched_once = {\"done\": False}  # mutable flag closed over by hook\n",
    "\n",
    "    def hook(module, inputs, output):\n",
    "        out = output.clone()  # [batch, seq_len, d_model]\n",
    "        seq_len = out.size(1)\n",
    "\n",
    "        # Only patch on first call with full prompt where v_idx is in range\n",
    "        if (not patched_once[\"done\"]) and (seq_len > v_idx):\n",
    "            out[:, v_idx, :] = v\n",
    "            patched_once[\"done\"] = True\n",
    "\n",
    "        return out\n",
    "\n",
    "    handle = emb.register_forward_hook(hook)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "\n",
    "def explain_feature_with_llama(v: torch.Tensor,\n",
    "                               layer: int,\n",
    "                               template_id: int | None = None,\n",
    "                               max_new_tokens: int = 96) -> str:\n",
    "    \"\"\"\n",
    "    v:      SAE feature direction in residual space, shape [hidden_size] (4096).\n",
    "    layer:  layer index ℓ for the textual prompt.\n",
    "    \"\"\"\n",
    "    prompt = make_feature_prompt(layer, template_id)\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    v_norm = normalize_feature(v)\n",
    "\n",
    "    with patch_at_v_between_s_e(model, input_ids, v_norm):\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03245d8a-f975-486e-a2c7-810086525d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_description_dataset import *\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "CACHE_DIR = \"/mnt/raid10/ak-research-01/ak-research-01/codes/.cache\"\n",
    "\n",
    "PROCESSED_ROOT = (\n",
    "    \"/mnt/raid10/ak-research-01/ak-research-01/codes/steer-vector/\"\n",
    "    \"latentqa/1_feature_description/dataset/processed_dataset\"\n",
    ")\n",
    "\n",
    "# Layers L01–L13, skipping L03\n",
    "layers = list(range(1, 14))\n",
    "if 3 in layers:\n",
    "    layers.remove(3)\n",
    "\n",
    "TRAIN_JSON_LIST = [\n",
    "    os.path.join(PROCESSED_ROOT, f\"L{layer:02d}\", \"train.jsonl\")\n",
    "    for layer in layers\n",
    "]\n",
    "\n",
    "TEST_JSON_LIST = [\n",
    "    os.path.join(PROCESSED_ROOT, f\"L{layer:02d}\", \"test.jsonl\")\n",
    "    for layer in layers\n",
    "]\n",
    "\n",
    "MAX_LENGTH = 128   # you can increase if explanations are long\n",
    "\n",
    "train_dataset = FeatureExplanationDataset(TRAIN_JSON_LIST, tokenizer, MAX_LENGTH)\n",
    "eval_dataset = FeatureExplanationDataset(TEST_JSON_LIST, tokenizer, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ba4d1ae-ac00-48a7-b21a-db6d274ff434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f33fd4-adfa-46db-a1c8-890b41fa68ba",
   "metadata": {},
   "source": [
    "## Test the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44a9431b-f388-4098-aab5-7954a528af60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompted explanation:\n",
      " We can describe v at layer 1 as encoding 0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x\n",
      "\n",
      "True explanation:\n",
      " modal verbs expressing ability or possibility\n"
     ]
    }
   ],
   "source": [
    "explanation = explain_feature_with_llama(torch.tensor(train_dataset.data[0][\"vector\"]), train_dataset.data[0]['layer'])\n",
    "print(\"Prompted explanation:\\n\", explanation)\n",
    "\n",
    "print(\"\\nTrue explanation:\\n\", train_dataset.data[0]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94baf2-d260-4ea6-ac47-dcc62b711901",
   "metadata": {},
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "924a71dc-714f-4e41-ade8-42c8183fad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/mnt/raid10/ak-research-01/ak-research-01/codes/steer-vector/latentqa/1_feature_description/model/explainer_manual_ckpt' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b5db1341704987a476966b2ddf552e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded explainer from: /mnt/raid10/ak-research-01/ak-research-01/codes/steer-vector/latentqa/1_feature_description/model/explainer_manual_ckpt\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "DATA_DIR = \"/mnt/raid10/ak-research-01/ak-research-01/codes/steer-vector/latentqa/1_feature_description/model\"\n",
    "CKPT_DIR = os.path.join(DATA_DIR, \"explainer_manual_ckpt\")\n",
    "\n",
    "# 1) Load tokenizer and model from the checkpoint directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(CKPT_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CKPT_DIR,\n",
    "    torch_dtype=torch.bfloat16,     # match what you used in training\n",
    "    device_map=\"auto\",              # or device=\"cuda\"\n",
    ")\n",
    "\n",
    "# 2) Ensure pad token is set (needed for padding and generation)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model.eval()   # for inference\n",
    "device = model.device\n",
    "print(\"Loaded explainer from:\", CKPT_DIR)\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44f31ad0-1ea7-40ff-8485-20c407e573d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompted explanation:\n",
      " At layer 2, v encodes the name \"Richard\" and variations of it\n",
      "\n",
      "True explanation:\n",
      " references to specific individuals, particularly those named \"Chadwick.\"\n"
     ]
    }
   ],
   "source": [
    "idx = 72\n",
    "\n",
    "explanation = explain_feature_with_llama(torch.tensor(eval_dataset.data[idx][\"vector\"]), eval_dataset.data[idx]['layer'])\n",
    "print(\"Prompted explanation:\\n\", explanation) \n",
    "\n",
    "print(\"\\nTrue explanation:\\n\", eval_dataset.data[idx]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62385bb-c46d-4dd1-8510-4beb846405f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
