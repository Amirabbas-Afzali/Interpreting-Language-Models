{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95ed861d-9ebe-479b-84d9-4ed69fc14032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "CACHE_DIR = \"/mnt/raid10/ak-research-01/ak-research-01/codes/.cache\"\n",
    "DATA_PATH = \"/mnt/raid10/ak-research-01/ak-research-01/codes/steer-vector/latentqa/3_input_ablation/generated_dataset/input_ablation_llama_instruct_train.jsonl\"\n",
    "\n",
    "EXPLAINER_MODEL_PATH = (\n",
    "    # \"/mnt/raid10/ak-research-01/ak-research-01/\"\n",
    "    # \"codes/steer-vector/latentqa/3_input_ablation/models/qwen\"\n",
    "    # \"meta-llama/Meta-Llama-3.1-8B-Instruct\"    \n",
    "    \"Qwen/Qwen3-8B\"\n",
    ")\n",
    "\n",
    "IS_QWEN = False          # True if explainer is Qwen-based\n",
    "VAL_SPLIT = 0.05\n",
    "SEED = 42\n",
    "\n",
    "MAX_NEW_TOKENS = 64\n",
    "BATCH_SIZE = 4           # adjust for VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38593453-c2f8-4b87-83d4-ed105abeddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce59ae90438942ff8963428121d589bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# LOAD MODEL & TOKENIZER\n",
    "# =========================\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tk_kwargs: Dict[str, Any] = {}\n",
    "model_kwargs: Dict[str, Any] = {}\n",
    "if IS_QWEN:\n",
    "    tk_kwargs[\"trust_remote_code\"] = True\n",
    "    model_kwargs[\"trust_remote_code\"] = True\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EXPLAINER_MODEL_PATH, **tk_kwargs)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    EXPLAINER_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    **model_kwargs,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD DATA (train/val split)\n",
    "# =========================\n",
    "\n",
    "raw_ds = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": DATA_PATH},\n",
    ")[\"train\"]\n",
    "\n",
    "ds = raw_ds.train_test_split(test_size=VAL_SPLIT, seed=SEED)\n",
    "# train_ds = ds[\"train\"]      # not used here, but kept for clarity\n",
    "val_ds = ds[\"test\"]         # evaluation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87a42389-52df-42fb-b796-cde8455aef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "\n",
    "LETTER_RE = re.compile(r\"Answer\\s*[:\\-]\\s*([ABCD])\", re.IGNORECASE)\n",
    "TRIPLE_ANGLE_RE = re.compile(r\"<<<\\s*Answer\\s*:\\s*([ABCD])\\s*>>>\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParsedExplanation:\n",
    "    has_changed: int\n",
    "    answer_letter: str\n",
    "\n",
    "\n",
    "def parse_explanation(text: str) -> ParsedExplanation:\n",
    "    lower = text.lower()\n",
    "\n",
    "    if \"remain unchanged\" in lower:\n",
    "        has_changed = 0\n",
    "    elif \"change to\" in lower or \"would change\" in lower or \"will change\" in lower:\n",
    "        has_changed = 1\n",
    "    else:\n",
    "        has_changed = 0\n",
    "\n",
    "    # Prefer <<<Answer: X>>>\n",
    "    m = TRIPLE_ANGLE_RE.search(text)\n",
    "    if m:\n",
    "        letter = m.group(1).upper()\n",
    "    else:\n",
    "        m2 = LETTER_RE.search(text)\n",
    "        if m2:\n",
    "            letter = m2.group(1).upper()\n",
    "        else:\n",
    "            m3 = re.search(r\"\\b([ABCD])\\b\", text)\n",
    "            letter = m3.group(1).upper() if m3 else \"\"\n",
    "\n",
    "    return ParsedExplanation(has_changed=has_changed, answer_letter=letter)\n",
    "\n",
    "\n",
    "def binary_f1(gold: List[int], pred: List[int]) -> float:\n",
    "    tp = sum(1 for g, p in zip(gold, pred) if g == 1 and p == 1)\n",
    "    fp = sum(1 for g, p in zip(gold, pred) if g == 0 and p == 1)\n",
    "    fn = sum(1 for g, p in zip(gold, pred) if g == 1 and p == 0)\n",
    "\n",
    "    if tp == 0:\n",
    "        return 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_batch(prompts: List[str]) -> List[str]:\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).to(device)\n",
    "\n",
    "    out = model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gens: List[str] = []\n",
    "    for i in range(len(prompts)):\n",
    "        prompt_len = enc[\"input_ids\"][i].shape[0]\n",
    "        gen_ids = out[i][prompt_len:]\n",
    "        text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "        gens.append(text.strip())\n",
    "    return gens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3da9b3d9-a357-4479-ac6f-3896270bf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# EVALUATION\n",
    "# =========================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate() -> None:\n",
    "    N = len(val_ds)\n",
    "\n",
    "    exact_flags: List[int] = []\n",
    "    gold_has_changed: List[int] = []\n",
    "    pred_has_changed: List[int] = []\n",
    "    gold_letters: List[str] = []\n",
    "    pred_letters: List[str] = []\n",
    "\n",
    "    for start in tqdm(range(0, N, BATCH_SIZE)):\n",
    "        end = min(start + BATCH_SIZE, N)\n",
    "        batch = val_ds[start:end]\n",
    "        prompts = batch[\"explainer_prompt\"]\n",
    "        preds = generate_batch(prompts)\n",
    "        gold_texts = batch[\"explainer_output\"]\n",
    "        has_changeds = batch[\"has_changed\"]\n",
    "        target_answer_nohints = batch[\"target_answer_nohint\"]\n",
    "\n",
    "        for example, has_changed, target_answer_nohint, pred_text in zip(gold_texts, has_changeds, target_answer_nohints, preds):\n",
    "            gold_text = example.strip()\n",
    "            \n",
    "            # exact match\n",
    "            exact_flags.append(int(pred_text.strip() == gold_text))\n",
    "\n",
    "            parsed = parse_explanation(pred_text)\n",
    "            pred_has_changed.append(parsed.has_changed)\n",
    "            pred_letters.append(parsed.answer_letter)\n",
    "\n",
    "            gold_has_changed.append(int(has_changed))\n",
    "            gold_letters.append(target_answer_nohint.strip().upper())\n",
    "\n",
    "    # Exact Match\n",
    "    exact_match = sum(exact_flags) / N * 100.0\n",
    "\n",
    "    # Has-Changed F1\n",
    "    has_changed_f1 = binary_f1(gold_has_changed, pred_has_changed) * 100.0\n",
    "\n",
    "    # Content Match: only where explainer predicts \"changed\"\n",
    "    correct_content = 0\n",
    "    denom_content = 0\n",
    "    for p_c, p_l, g_l in zip(pred_has_changed, pred_letters, gold_letters):\n",
    "        if p_c == 1:\n",
    "            denom_content += 1\n",
    "            if p_l == g_l:\n",
    "                correct_content += 1\n",
    "    content_match = (\n",
    "        100.0 * correct_content / denom_content if denom_content > 0 else 0.0\n",
    "    )\n",
    "\n",
    "    print(\"===== Input Ablation Explainer Evaluation (val split) =====\")\n",
    "    print(f\"#examples (val): {N}\")\n",
    "    print(f\"Exact Match:     {exact_match:.2f}%\")\n",
    "    print(f\"Has-Changed F1:  {has_changed_f1:.2f}%\")\n",
    "    print(f\"Content Match:   {content_match:.2f}% \"\n",
    "          f\"(on {denom_content} predicted 'changed' examples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdcc0011-f33b-4dc5-85e3-26f948e11f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 155/155 [05:31<00:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input Ablation Explainer Evaluation (val split) =====\n",
      "#examples (val): 620\n",
      "Exact Match:     0.00%\n",
      "Has-Changed F1:  3.66%\n",
      "Content Match:   16.67% (on 12 predicted 'changed' examples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9ccff-fcbe-4407-88be-f4a6563fd852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff339e-889b-491a-ab51-9bb31de94e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sv2)",
   "language": "python",
   "name": "sv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
